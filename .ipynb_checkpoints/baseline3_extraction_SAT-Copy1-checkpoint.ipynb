{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import baselines as b\n",
    "from sklearn.metrics import accuracy_score,f1_score, recall_score, precision_score, average_precision_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gt=pd.read_csv('./data/impr_find/train_input_labelled.csv')\n",
    "test=pd.read_csv('./data/impr_find/test_input_labelled.csv')\n",
    "\n",
    "cleaned_reports=pd.read_csv('./data/impr_find/cleaned_test_reports.csv') #### to be used at the end to calcuate bleu scores with respect extracted baseline3 report\n",
    "\n",
    "gt=gt.fillna(0.0)\n",
    "\n",
    "\n",
    "gt=gt.replace(-1,0)\n",
    "\n",
    "test=test.fillna(0.0)\n",
    "\n",
    "\n",
    "test=test.replace(-1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "2534\n",
      "292\n",
      "199 0.9097981734130116\n",
      "Reports    Cardiomediastinal silhouette is within normal ...\n",
      "Name: 199, dtype: object\n",
      "(250, 14)\n",
      "accuracy: 0.9134285714285716\n",
      "precision_class wise [0.392 0.04  0.136 0.036 0.22  0.028 0.06  0.036 0.068 0.048 0.064 0.008\n",
      " 0.036 0.04 ]\n",
      "precision_macro 0.08657142857142859\n",
      "recall_macro 0.0\n",
      "precision_micro 0.08657142857142858\n",
      "recall micro 0.0\n"
     ]
    }
   ],
   "source": [
    "##### Chexpert based baseline extraction\n",
    "\n",
    "# accuracy_row_wise=[]\n",
    "# f1_row_wise=[]\n",
    "# prec_row_wise=[]\n",
    "# recl_row_wise=[]\n",
    "# final_accuracy_score=[]\n",
    "# final_prec_score=[]\n",
    "# final_recl_score=[]\n",
    "# for base in range(0,(gt.shape[0])):\n",
    "    \n",
    "#     accuracy_row_wise=[]\n",
    "#     prec_row_wise=[]\n",
    "#     recl_row_wise=[]\n",
    "#     for r in range(0,(gt.shape[0])):\n",
    "\n",
    "#         g=list(gt.iloc[r,1:])\n",
    "#         #print(g)\n",
    "#         c=list(gt.iloc[base,1:])\n",
    "#         #print(c)\n",
    "#         accuracy_row_wise.append(accuracy_score(c,g))\n",
    "#         f1_row_wise.append(f1_score(c, g))\n",
    "#         prec_row_wise.append((precision_score(c,g)))\n",
    "#         recl_row_wise.append(recall_score(c,g))\n",
    "#     final_accuracy_score.append(np.mean(accuracy_row_wise))\n",
    "#     final_prec_score.append(np.mean(prec_row_wise))\n",
    "#     final_recl_score.append(np.mean(recl_row_wise))\n",
    "  \n",
    "# print(np.argmax(final_accuracy_score))\n",
    "# print(np.max(final_accuracy_score))\n",
    "# print(np.max(final_prec_score))\n",
    "# print(np.max(final_recl_score))\n",
    "# print(gt.iloc[np.argmax(final_accuracy_score),0])\n",
    "\n",
    "\n",
    "########## Through Manual method\n",
    "\n",
    "def one_out_of_best(accuracies):\n",
    "    best_accuracy=np.max(accuracy)\n",
    "    \n",
    "    best_reports_idx= [idx for idx,value in enumerate(accuracy) if value==best_accuracy]\n",
    "    \n",
    "    print(len(best_reports_idx))\n",
    "    \n",
    "    random_best_idx=random.choice(best_reports_idx)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return random_best_idx\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def extract_report(train_data):\n",
    "\n",
    "    n=train_data.shape[0]\n",
    "    #print(type(train_data))\n",
    "    accuracy=[]\n",
    "\n",
    "    #print(train_data)\n",
    "\n",
    "\n",
    "    for index in range(0,n):\n",
    "        if (index%500==0):\n",
    "            print(index)\n",
    "\n",
    "        pred=train_data.iloc[index,:]\n",
    "\n",
    "        pred=pred*1.0\n",
    "        #accuracy.append(accuracy_score(train_data,pred))\n",
    "        comp=train_data.values* pred.values + (1 - train_data.values) * (1 - pred.values)\n",
    "        accu = np.mean(comp,axis=1)\n",
    "\n",
    "        accuracy.append(np.mean(accu))\n",
    "        #print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "data=gt.iloc[:,1:]\n",
    "test=test.iloc[:,1:]\n",
    "\n",
    "\n",
    "accuracy=extract_report(data)\n",
    "print(len(accuracy))\n",
    "\n",
    "#### select random report out of best reports with respect to accuracy\n",
    "\n",
    "report_id=one_out_of_best(accuracy)\n",
    "\n",
    "print(report_id, accuracy[report_id])\n",
    "print(gt.iloc[report_id,:1])\n",
    "report=gt.iloc[report_id,:1]\n",
    "##########\n",
    "\n",
    "######## selecting report using argamx function\n",
    "# report_id=np.argmax(accuracy)\n",
    "\n",
    "# print(report_id)\n",
    "# print(gt.iloc[report_id,:1])\n",
    "# print(np.max(accuracy))\n",
    "\n",
    "##################\n",
    "\n",
    "########### To check top reports\n",
    "# print(len([i for i in accuracy if i>0.90]))\n",
    "\n",
    "# gt['accuracy']=accuracy\n",
    "\n",
    "# gt=gt.sort_values(by=['accuracy'],ascending=False)\n",
    "\n",
    "# print(gt.head(10))\n",
    "\n",
    "##############################\n",
    "n=test.shape[0]\n",
    "r=pd.DataFrame(data.iloc[report_id,:]).T\n",
    "\n",
    "pred = r * 1.0\n",
    "\n",
    "\n",
    "\n",
    "# print(pred.columns)\n",
    "# print(test.columns)\n",
    "# accuracy.append(accuracy_score(train_data,pred))\n",
    "comp = test.values * pred.values + (1 - test.values) * (1 - pred.values)\n",
    "\n",
    "print(comp.shape)\n",
    "# print(comp.head(3))\n",
    "# print(comp.tail(3))\n",
    "accu = np.mean(comp, axis=1)\n",
    "\n",
    "print('accuracy:', np.mean(accu))\n",
    "\n",
    "pred1 = pd.concat([r] * n, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# accu1=accuracy_score(test,pred1)\n",
    "\n",
    "# print('accuracy:', accu1)\n",
    "\n",
    "\n",
    "\n",
    "print('precision_class wise', average_precision_score(test, pred1, average=None))\n",
    "print('precision_macro',average_precision_score(test,pred1, average='macro'))\n",
    "print('recall_macro',recall_score(test,pred1, average='macro'))\n",
    "print('precision_micro',average_precision_score(test,pred1, average='micro'))\n",
    "print('recall micro',recall_score(test,pred1, average='micro'))\n",
    "\n",
    "print(b.all_scores(report,cleaned_report['captions'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
