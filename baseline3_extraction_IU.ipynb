{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import baselines as b\n",
    "from sklearn.metrics import accuracy_score,f1_score, recall_score, precision_score, average_precision_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./data/impr_find/Sample'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########## Through Manual method\n",
    "\n",
    "def one_out_of_best(accuracies):\n",
    "    best_accuracy=np.max(accuracy)\n",
    "    \n",
    "    best_reports_idx= [idx for idx,value in enumerate(accuracy) if value==best_accuracy]\n",
    "    \n",
    "    print(len(best_reports_idx))\n",
    "    \n",
    "    random_best_idx=random.choice(best_reports_idx)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return random_best_idx\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def extract_report(train_data):\n",
    "\n",
    "    n=train_data.shape[0]\n",
    "    #print(type(train_data))\n",
    "    accuracy=[]\n",
    "\n",
    "    #print(train_data)\n",
    "\n",
    "\n",
    "    for index in range(0,n):\n",
    "        if (index%500==0):\n",
    "            print(index)\n",
    "\n",
    "        pred=train_data.iloc[index,:]\n",
    "\n",
    "        pred=pred*1.0\n",
    "        #accuracy.append(accuracy_score(train_data,pred))\n",
    "        comp=train_data.values* pred.values + (1 - train_data.values) * (1 - pred.values)\n",
    "        accu = np.mean(comp,axis=1)\n",
    "\n",
    "        accuracy.append(np.mean(accu))\n",
    "        #print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 2\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "2534\n",
      "288\n",
      "1131 0.9099391137670539\n",
      "Reports    calcified left hilar lymph node lungs are clea...\n",
      "Name: 1131, dtype: object\n",
      "accuracy: 0.9077142857142858\n",
      "{'testlen': 8250, 'reflen': 9554, 'guess': [8250, 8000, 7750, 7500], 'correct': [3247, 1302, 582, 207]}\n",
      "ratio: 0.8635126648523274\n",
      "sample: 3\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "2534\n",
      "289\n",
      "861 0.9098545495546284\n",
      "Reports    right lower lobe xxxx calcified granuloma hear...\n",
      "Name: 861, dtype: object\n",
      "accuracy: 0.9085714285714287\n",
      "{'testlen': 6500, 'reflen': 9734, 'guess': [6500, 6250, 6000, 5750], 'correct': [2416, 817, 297, 66]}\n",
      "ratio: 0.6677624820217107\n",
      "sample: 4\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "2534\n",
      "296\n",
      "1519 0.9096290449881612\n",
      "Reports    mild hyperinflation no focal consolidation sus...\n",
      "Name: 1519, dtype: object\n",
      "accuracy: 0.9108571428571429\n",
      "{'testlen': 8250, 'reflen': 9162, 'guess': [8250, 8000, 7750, 7500], 'correct': [2723, 820, 230, 56]}\n",
      "ratio: 0.9004584151930909\n",
      "sample: 5\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "2534\n",
      "286\n",
      "2318 0.9097699853422033\n",
      "Reports    there are scattered calcified granulomas no fo...\n",
      "Name: 2318, dtype: object\n",
      "accuracy: 0.9094285714285716\n",
      "{'testlen': 8500, 'reflen': 9638, 'guess': [8500, 8250, 8000, 7750], 'correct': [3387, 1140, 494, 226]}\n",
      "ratio: 0.8819257107282753\n",
      "0.3015225875683648 0.1800109055670816 0.11169326153529219 0.06746353283194413 0.1457790685149074 0.23825031821699008 0.16013889064649056\n",
      "0.047805827479538494 0.03274186537949287 0.026668663078963876 0.02207796766555012 0.009004868254710077 0.02204760382116388 0.026925978897787414\n"
     ]
    }
   ],
   "source": [
    "sample=[2,3,4,5]\n",
    "\n",
    "bleu1,bleu2,bleu3,bleu4,M, R, C=[],[],[],[],[],[],[]\n",
    "\n",
    "for i in sample:\n",
    "    \n",
    "    print('sample:', i)\n",
    "    gt=pd.read_csv(path+str(i)+'/train_labelled.csv')\n",
    "    test=pd.read_csv(path+str(i)+'/test_labelled.csv')\n",
    "\n",
    "    gt=gt.fillna(0.0)\n",
    "\n",
    "\n",
    "    gt=gt.replace(-1,0)\n",
    "\n",
    "    test=test.fillna(0.0)\n",
    "\n",
    "\n",
    "    test=test.replace(-1,0)\n",
    "    \n",
    "    data=gt.iloc[:,1:]\n",
    "\n",
    "    test_data=test.iloc[:,1:]\n",
    "\n",
    "\n",
    "    accuracy=extract_report(data)\n",
    "    print(len(accuracy))\n",
    "\n",
    "    #### select random report out of best reports with respect to accuracy\n",
    "\n",
    "    report_id=one_out_of_best(accuracy)\n",
    "\n",
    "    print(report_id, accuracy[report_id])\n",
    "    print(gt.iloc[report_id,:1])\n",
    "    report=gt.iloc[report_id,:1]\n",
    "    ##########\n",
    "\n",
    "    ######## selecting report using argamx function\n",
    "    # report_id=np.argmax(accuracy)\n",
    "\n",
    "    # print(report_id)\n",
    "    # print(gt.iloc[report_id,:1])\n",
    "    # print(np.max(accuracy))\n",
    "\n",
    "    ##################\n",
    "\n",
    "    ########### To check top reports\n",
    "    # print(len([i for i in accuracy if i>0.90]))\n",
    "\n",
    "    # gt['accuracy']=accuracy\n",
    "\n",
    "    # gt=gt.sort_values(by=['accuracy'],ascending=False)\n",
    "\n",
    "    # print(gt.head(10))\n",
    "\n",
    "    ##############################\n",
    "    n=test_data.shape[0]\n",
    "    r=pd.DataFrame(data.iloc[report_id,:]).T\n",
    "\n",
    "    pred = r * 1.0\n",
    "\n",
    "\n",
    "    comp = test_data.values * pred.values + (1 - test_data.values) * (1 - pred.values)\n",
    "\n",
    "    #print(comp.shape)\n",
    "    \n",
    "    accu = np.mean(comp, axis=1)\n",
    "\n",
    "    print('accuracy:', np.mean(accu))\n",
    "\n",
    "#     pred1 = pd.concat([r] * n, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#     # accu1=accuracy_score(test,pred1)\n",
    "\n",
    "#     # print('accuracy:', accu1)\n",
    "\n",
    "\n",
    "\n",
    "#     print('precision_class wise', average_precision_score(test, pred1, average=None))\n",
    "#     print('precision_macro',average_precision_score(test,pred1, average='macro'))\n",
    "#     print('recall_macro',recall_score(test,pred1, average='macro'))\n",
    "#     print('precision_micro',average_precision_score(test,pred1, average='micro'))\n",
    "#     print('recall micro',recall_score(test,pred1, average='micro'))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    bleus,meteor, rouge, ciders=b.all_scores(test.iloc[:,0],report[0])\n",
    "    \n",
    "    bleu1.append(bleus[0])\n",
    "    bleu2.append(bleus[1])\n",
    "    bleu3.append(bleus[2])\n",
    "    bleu4.append(bleus[3])\n",
    "    M.append(meteor)\n",
    "    R.append(rouge)\n",
    "    C.append(ciders)\n",
    "\n",
    "print(np.mean(bleu1),np.mean(bleu2),np.mean(bleu3),np.mean(bleu4),np.mean(M),np.mean(R),np.mean(C))\n",
    "print(np.std(bleu1),np.std(bleu2),np.std(bleu3),np.std(bleu4),np.std(M),np.std(R),np.std(C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
